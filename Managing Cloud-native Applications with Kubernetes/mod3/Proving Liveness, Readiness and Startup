<!-- ----------------------------------------------------------------------- -->
<!--                 application is ready to serve requests                  -->
<!-- ----------------------------------------------------------------------- -->

proving whether an application is ready to serve requests in a Kubernetes environment. Key points include:

Current Limitations: Previously, the only status check was whether the node hosting the application was alive, without a way to assess the application's specific state.
Need for Granular Status Checks: The lecture emphasizes the importance of determining various states of the application, such as temporary connection losses, application errors, and configuration errors.
Kubernetes Probes: The lecture introduces three types of probes to check application status:
Startup Probe: Runs once at the beginning of the pod's lifecycle to determine if the application is ready to serve requests.
Readiness Probe: Continuously checks if the pod is ready to handle requests. If it fails, requests are not routed to the pod.
Liveness Probe: Detects unhealthy states within the application. If it fails, the pod is restarted.
These probes help ensure that applications are functioning correctly and can handle requests effectively. 

<!-- ----------------------------------------------------------------------- -->
<!--              How do readiness and liveness probes differ?               -->
<!-- ----------------------------------------------------------------------- -->
Readiness Probes and Liveness Probes serve different purposes in Kubernetes:

Readiness Probe:
Purpose: Determines if a pod is ready to serve requests.
Execution: Runs continuously throughout the pod's lifecycle.
Behavior: If it fails, the pod is marked as not ready, and no requests are routed to it. Once it passes, the pod is considered ready to handle requests.

Liveness Probe:
Purpose: Detects if a pod is in an unhealthy state.
Execution: Also runs continuously throughout the pod's lifecycle.
Behavior: If it fails, the pod is restarted to recover from the unhealthy state.
In summary, the readiness probe checks if the application can handle requests, while the liveness probe checks if the application is still running properly and needs to be restarted.

<!-- ----------------------------------------------------------------------- -->
<!--                   if both probes fail simultaneously?                   -->
<!-- ----------------------------------------------------------------------- -->
If both the readiness probe and the liveness probe fail simultaneously, the following would typically occur:

Readiness Probe Failure:
The pod would be marked as not ready to serve requests. This means that Kubernetes would stop routing traffic to this pod, preventing any new requests from being handled.

Liveness Probe Failure:
The pod would be considered unhealthy, leading Kubernetes to restart the pod. This action is taken to recover the application from its unhealthy state.

In this scenario, the pod would stop receiving traffic due to the readiness probe failure, and then it would be restarted due to the liveness probe failure. Once the pod restarts, it would go through the startup probe (if configured) and then the readiness probe again to determine if it can start serving requests.

<!-- ----------------------------------------------------------------------- -->
<!--             If a pod keeps restarting due to probe failures             -->
<!-- ----------------------------------------------------------------------- -->

If a pod keeps restarting due to probe failures, you can take the following steps to troubleshoot and resolve the issue:

Check Logs:

Review the pod's logs to identify any error messages or issues that may be causing the application to fail. Use the command:
kubectl logs <pod-name>
Examine Probe Configuration:

Verify the configuration of the readiness and liveness probes. Ensure that the probe settings (e.g., timeout, initial delay, failure threshold) are appropriate for your application.
Increase Timeouts:

If the application takes longer to start or respond, consider increasing the timeout values for the probes.
Debug the Application:

Investigate the application code for potential issues, such as memory leaks, unhandled exceptions, or resource constraints that could lead to failures.
Resource Allocation:

Check if the pod has sufficient resources (CPU and memory). If not, consider adjusting the resource requests and limits.
Test Locally:

If possible, run the application locally or in a development environment to replicate the issue and debug it more easily.
Review Dependencies:
Ensure that any external dependencies (like databases or APIs) are available and functioning correctly, as their unavailability can cause the application to fail.

<!-- ----------------------------------------------------------------------- -->
<!--                           real-life examples                            -->
<!-- ----------------------------------------------------------------------- -->
Example 1: E-commerce Application

Scenario: An e-commerce application is deployed on Kubernetes, and it relies on a database to process transactions.
Readiness Probe: The readiness probe checks if the application can connect to the database and is ready to handle user requests. If the database is temporarily down for maintenance, the readiness probe will fail, and the application will not receive any traffic until the database is back online.
Liveness Probe: The liveness probe checks if the application is running without issues. If there is a memory leak causing the application to freeze, the liveness probe will fail, triggering a restart of the pod to recover from the unhealthy state.

Example 2: Online Streaming Service

Scenario: A video streaming service is hosted on Kubernetes, serving thousands of users.
Readiness Probe: The readiness probe ensures that the streaming service is fully initialized and can handle user requests. If the service is still loading content or initializing, the readiness probe will fail, preventing users from accessing the service until it is ready.
Liveness Probe: The liveness probe monitors the health of the streaming service. If the service encounters an error that causes it to stop processing requests (e.g., a bug in the code), the liveness probe will fail, and Kubernetes will restart the pod to restore functionality.

Example 3: Microservices Architecture

Scenario: A microservices-based application consists of multiple services communicating with each other.
Readiness Probe: Each microservice has its own readiness probe to ensure it can communicate with other services. If one service is down, its readiness probe will fail, and requests will not be routed to it, preventing cascading failures across the system.
Liveness Probe: If a microservice encounters an issue that causes it to hang (e.g., waiting indefinitely for a response), the liveness probe will fail, and Kubernetes will restart that microservice to maintain overall system health.
These examples illustrate how readiness and liveness probes play a crucial role in maintaining the reliability and availability of applications in a Kubernetes environment. 

<!-- ----------------------------------------------------------------------- -->
<!--       Example: Managing a Web Application in a Cloud Environment        -->
<!-- ----------------------------------------------------------------------- -->

Scenario: You are responsible for deploying a web application that handles user requests and connects to a database. The application is hosted on a Kubernetes cluster.

Startup Probe:

Relevance: When you deploy the application, it may take some time for the database to initialize and be ready to accept connections.
Application: You configure a startup probe that checks if the database is ready. If the probe fails (e.g., the database isn't ready yet), Kubernetes will not route any requests to the application until the probe passes, ensuring that users do not encounter errors.
Readiness Probe:

Relevance: Once the application is running, you want to ensure it can handle incoming requests.
Application: You set up a readiness probe that continuously checks the application’s health. If the application becomes unresponsive (e.g., due to high load), the probe will fail, and Kubernetes will stop sending requests to it until it is ready again. This prevents users from experiencing downtime.
Liveness Probe:
Relevance: Over time, the application might encounter issues like memory leaks that cause it to freeze without crashing.
Application: You implement a liveness probe that checks the application's health at regular intervals. If the probe fails, Kubernetes will automatically restart the application, ensuring that it remains available to users without manual intervention.

<!-- ----------------------------------------------------------------------- -->
<!--          configure the failure threshold for a liveness probe           -->
<!-- ----------------------------------------------------------------------- -->
You can configure the failure threshold for a liveness probe in Kubernetes by using the failureThreshold parameter in the probe's configuration. This parameter specifies the number of consecutive failures required for the probe to be considered failed, which will trigger a restart of the pod.

Example Configuration:

Here’s an example of a liveness probe configuration in a Kubernetes deployment YAML file:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app-image:latest
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 3
Breakdown of the Configuration:

httpGet: This specifies that the probe will perform an HTTP GET request to the /health endpoint on port 8080.
initialDelaySeconds: This is the time to wait before starting the probe after the container has started (10 seconds in this case).
periodSeconds: This defines how often (in seconds) to perform the probe (every 5 seconds here).
failureThreshold: This is set to 3, meaning that if the liveness probe fails three consecutive times, Kubernetes will consider the pod unhealthy and restart it.
This configuration helps ensure that your application remains healthy and can recover from issues automatically. 

<!-- ----------------------------------------------------------------------- -->
<!--     adjust the parameters of a readiness probe for an application.      -->
<!-- ----------------------------------------------------------------------- -->
Scenario: Deploying a Data-Intensive Web Application

Context: You are deploying a web application that processes large datasets and requires significant time to initialize and load data into memory before it can handle user requests effectively.

Initial Configuration:

The readiness probe is configured with default parameters:
initialDelaySeconds: 5
periodSeconds: 10
failureThreshold: 3
Issue: 

After deploying the application, you notice that the readiness probe is failing frequently during the initial loading phase. The application takes longer than 5 seconds to load the necessary data, causing the probe to fail and marking the pod as "not ready." As a result, no traffic is routed to the application, leading to downtime for users.
Adjustments Needed:

Increase initialDelaySeconds:

Adjustment: Change initialDelaySeconds to 30.
Reason: This gives the application more time to initialize before the readiness probe starts checking its health.
Adjust periodSeconds:

Adjustment: Change periodSeconds to 15.
Reason: This reduces the frequency of checks, allowing the application more time to stabilize between checks.
Modify failureThreshold:
Adjustment: Change failureThreshold to 5.
Reason: This allows for more consecutive failures before the pod is marked as "not ready," accommodating temporary issues during the loading phase.
Result:

By making these adjustments, the readiness probe will now wait longer before starting checks, check less frequently, and tolerate more failures. This ensures that the application has sufficient time to load data and become ready to serve requests, ultimately improving user experience and reducing downtime.